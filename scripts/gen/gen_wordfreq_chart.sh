#!/bin/bash
# gen_wordfreq_chart.sh

# Generates a list of ALL of the words used in ALL of the transcripts. This will ultimately be used to make the final TSV file. Given that we already have frequencies, I decided I may as well use awk to get the grand total sum of each word's frequency in all of the State of the Union addresses.

# Argument 1: (Input) Directory full of frequency lists (generated by wordfreq.sh)
# Argument 2: (Input) Master wordlist (generated by gen_master_wordlist.sh)
# Argument 3: Output directory

loading_updates=10 # after how many WORDS have been processed do we report to the console.
output_filename="wordfreq_chart.tsv"

# ----------------
script_name=$(basename "$0")

# Ensuring we have the files to work with
if [ ! -d "${1}" ] || [ ! -f "${2}" ] || [ ! $# -ne 4 ]; then
	echo "[${script_name}]: The correct argument structure is: (1) path to directory full of frequency lists, (2) path to master wordlist file, (3) output directory location. "
	exit 1
fi

# Grab the text files, ensure they exist

DIR_input="$1"
DIR_output="$3"
master_wordlist_fp="$2"
output_file="${DIR_output}/${output_filename}"
freq_files=(${DIR_input}/*.tsv)

if [ "${freq_files[0]}" == "${DIR_input}/*.tsv" ]; then
	# If equal, there was no glob expansion (no text files), and the string literal is preserved.
	echo "[${script_name}]: The freq directory must contain at least one tsv file. Terminating."
	exit 1
fi

[[ -d "$DIR_output" ]] || mkdir -p "$DIR_output"

if [[ -f $output_file ]]; then
	echo "[${script_name}]: Word frequency table already exists in this directory. Regenerate? (y/n)"
	read response
	if [[ "$response" != "y" && "$response" != "Y" ]]; then
		exit 0
	else
		mv --backup=t "${DIR_output}/${output_filename}" "${DIR_output}/${output_filename}_old" # won't be overwritten
		#rm "${DIR_output}/${output_filename}"
	fi
fi

echo "[${script_name}]: Working..."

# -------------------------------------

# header
header="Word"
for file in "${freq_files[@]}"; do
	header+=$'\t'
	header+="$(basename "$file")"
done

echo -e "$header" >> "$output_file"

row_text=""
master_wordlist=$(cat $master_wordlist_fp)
 
# I think it would have been more efficient to work column-by-column (much fewer file openings), but I couldn't think of an easy way to do it, and so efficiency will have to take a backseat for now.
# So the outer loop takes a word from the master word list, and starts the line with it. Second value is ignored because it's just the overall frequencies.
count=0

while IFS=$'\t' read -r word ignore; do
	(( count++ ))
	(( count % loading_updates == 0 )) && echo "[${script_name}]: Still working... on the word $word"

	row_text+="$word"
	
	# Now the inner loop will check each file in order. To slightly improve the efficiency, I just opted to use awk instead of iterating line-by-line. 
	# I'm not mega comfortable with awk yet (this took a ton of research!) 
	# So, for each line, awk will first check if the first column is equal to the word. If so, print the value in the second column. If not, check if the word has already been passed by comparison (strings can be compared like ints). If it has, we can exit. If we've reached the end without any comparison, then the freq is 0
	for file in "${freq_files[@]}"; do
		freq=$(awk -v word="$word" '
		BEGIN {found=0}
		$1 == word {print $2; found = 1; exit}
		$1 > word {exit}
		END {if (found == 0) print 0}' "$file")
		#echo "frequency of word here is $freq(!)"
		row_text+="\t"
		row_text+="$freq"
		#echo "row_text is now $row_text"
		#echo "run"
	done

	#echo -e "outputting: $row_text"
	#sleep 2
	echo -e "$row_text" >> "$output_file"
	row_text=""
	#echo "----------------------------------"
done < <(echo -e "$master_wordlist")

echo "reached the end"
